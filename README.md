# AG-Pipeline-para-IA-Generativa-no-Lakehouse-Databricks

Vamos trabalhar  com um  dos  temas  mais  quentes  do  momento  no universo da Intelig√™ncia Artificial e implementar um RAG Pipeline com LLM do Databricks.

A Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) √© uma t√©cnica que aprimora modelos de linguagem de grande escala (LLMs) ao integr√°-los com fontes externas de conhecimento. Ao  receber  uma  consulta,  o  sistema  busca  informa√ß√µes  relevantes  em  bases  de  dados atualizadas. Essas informa√ß√µes s√£o ent√£o combinadas com a entrada original e processadas pelo modelo de linguagem para gerar uma resposta mais precisa e contextualizada. Essa  abordagem  oferece  diversas vantagens,  como  a  capacidade  de  fornecer  respostas atualizadas  sem  a  necessidade  de  retreinar  o  modelo,  redu√ß√£o  de  "alucina√ß√µes"  (respostas incorretas geradas pelo modelo) e maior controle sobre as fontes de informa√ß√£o utilizadas. O  RAG  √©  especialmente  √∫til  em  aplica√ß√µes  que  exigem  informa√ß√µes  atualizadas  ou espec√≠ficas  de  um  dom√≠nio,  como  chatbots  de  suporte  ao  cliente  ou  assistentes  virtuais  em setores especializados.

# Rela√ß√£o entre RAG e IA Generativa

A   rela√ß√£o   entre   RAG   (Retrieval   Augmented   Generation)   e   IA   Generativa   √©   de complementaridade. Ambos s√£o componentes que, juntos, formam uma abordagem poderosa para  gerar  respostas  mais  informadas,  contextuais  e  confi√°veis  em  sistemas  de  Intelig√™ncia Artificial.IA  Generativa:  Modelos  de  IA  Generativa,  como  os  LLMs  (Large  Language  Models), s√£o treinados  para  gerar  texto  com  base  em  padr√µes  aprendidos  a  partir  de  grandes  volumes  de dados. Embora sejam muito eficazes em tarefas de linguagem natural, eles t√™m limita√ß√µes, como a possibilidade de "alucina√ß√µes" (gerar informa√ß√µes erradas ou fict√≠cias) e depend√™ncia de dados de treinamento que podem estar desatualizados.RAG  como  Solu√ß√£o  para  Limita√ß√µes:  A  t√©cnica  de  RAG  integra  a  IA  Generativa  com mecanismos  de  recupera√ß√£o  de  informa√ß√µes.  Quando  uma  consulta  √©  feita,  o  sistema  busca informa√ß√µes relevantes em bases de dados externas ou espec√≠ficas (como documentos, bancos de  conhecimento  ou  at√©  mesmo  a  web).  Essas  informa√ß√µes  s√£o  ent√£o  usadas  como  contexto adicional para a IA Generativa, melhorando a precis√£o e relev√¢ncia das respostas.

üõ† Funcionamento Combinado:

* O  mecanismo  de  recupera√ß√£o  (retrieval)  identifica  dados  relevantes  em  tempo real a partir de um banco de dados vetorial.

* A  IA  Generativa  processaesses  dados  junto  com  a  entrada  original  do  usu√°rio, gerando uma resposta que combina o conhecimento do modelo com informa√ß√µes recuperadas.

üõ† Benef√≠cios da Integra√ß√£o:

* Atualiza√ß√£o  Cont√≠nua:  O  RAG  permite  que  os  modelos  generativos  acessem informa√ß√µes atualizadas sem necessidade de serem retreinados.

* Maior  Confiabilidade:  As  respostas  s√£o  mais  embasadas  em  fontes  externas verific√°veis, reduzindo erros.

* Adaptabilidade:  √â  poss√≠vel  personalizar  o  sistema  para  diferentes  dom√≠nios  ou necessidades espec√≠ficas.

Exemplo  Pr√°tico:  Imagine  um  chatbot  que  responde  perguntas  sobre  legisla√ß√£o.  Sem  o RAG,  ele  pode  fornecer  respostas  desatualizadas  baseadas  apenas  nos  dados  de  treinamento. Com  o  RAG,  o  sistema  pode  consultar  fontes  legais  atualizadas  antes  de  gerar  uma  resposta, garantindo maior precis√£o.

Neste projeto vamos construir um RAG Pipeline para uma aplica√ß√£o de IA Generativa. Usamos um LLM disponibilizado na plataforma Databricks e executaremos o Lab no Google Colab. Todos os arquivos est√£o dispon√≠vel.

# Por que usar RAG?

Usar RAG (Retrieval-Augmented Generation) em uma aplica√ß√£o de IA Generativa tem como principal objetivo aumentar a precis√£o, a confiabilidade e a atualidade das respostas geradas por modelos de linguagem (LLMs). Aqui est√£o os principais motivos pelos quais o uso do RAG √© extremamente valioso:

üéØ 1. Respostas mais precisas e baseadas em fatos
Problema: Modelos LLMs podem gerar alucina√ß√µes, ou seja, inventar informa√ß√µes que parecem corretas, mas s√£o falsas.

Solu√ß√£o com RAG: O sistema busca documentos reais e confi√°veis (por exemplo, PDFs, bases jur√≠dicas, artigos t√©cnicos) e fornece essas informa√ß√µes como contexto adicional ao modelo. Isso aumenta a veracidade da resposta.

üîÑ 2. Informa√ß√µes sempre atualizadas
Problema: LLMs s√£o treinados com dados de corte (ex: at√© 2023) e n√£o t√™m acesso a novas informa√ß√µes.

Solu√ß√£o com RAG: O pipeline pode consultar fontes atualizadas (como bancos de dados vetoriais com documentos novos) em tempo real, sem a necessidade de retreinar o modelo.

üß† 3. Personaliza√ß√£o por dom√≠nio
Problema: LLMs gen√©ricos n√£o t√™m conhecimento detalhado de dom√≠nios espec√≠ficos (como direito tribut√°rio, medicina ou engenharia).

Solu√ß√£o com RAG: O sistema pode ser alimentado com dados de dom√≠nio espec√≠ficos, como documentos internos de uma empresa ou artigos cient√≠ficos, adaptando o comportamento do modelo a contextos muito espec√≠ficos.

‚úÖ 4. Controle das fontes de informa√ß√£o
Com RAG, √© poss√≠vel saber de onde veio cada trecho de informa√ß√£o usado para gerar uma resposta. Isso permite maior auditoria e rastreabilidade, o que √© essencial em ambientes regulados (como jur√≠dico ou sa√∫de).

üí∞ 5. Redu√ß√£o de custos com retreinamento
Ao inv√©s de gastar tempo e dinheiro para retreinar um LLM com dados novos, voc√™ pode apenas atualizar a base de conhecimento consultada pelo RAG (por exemplo, adicionar novos arquivos √† base vetorial).

# üß∞ Ferramentas Utilizadas

‚úÖ Ollama

‚úÖ Databricks

‚úÖ Jupyter notabook

![image](https://github.com/user-attachments/assets/0931b8e6-a6bc-4e1d-b657-36d38f82b3fc)


# Vamos iniciar o projeto inicializando o Jupytrt Notebook:


üì¶ 1. Instala√ß√£o de Pacotes

Os pacotes necess√°rios s√£o instalados com pip, incluindo:

* LangChain: Framework para orquestra√ß√£o de LLMs.

* HuggingFace: Para embeddings.

* Milvus: Base vetorial para armazenamento e busca sem√¢ntica.

* Databricks CLI e SQL Connector: Integra√ß√£o com o Databricks.

* BeautifulSoup (bs4): Para extrair textos de p√°ginas HTML.



üîê 2. Configura√ß√£o de Credenciais do Databricks

As credenciais do workspace do Databricks s√£o definidas com vari√°veis de ambiente:

os.environ["DATABRICKS_HOST"] = "url_Databricks"
os.environ["DATABRICKS_TOKEN"] = "token"

Essas credenciais permitem que voc√™ use o LLM hospedado no Databricks.

![image](https://github.com/user-attachments/assets/b0548a6e-6ad3-45b7-9171-1f3e19099e06)
"Host"

![image](https://github.com/user-attachments/assets/0a5349e5-a4c8-426d-9a48-7256ddf99082)
"Tokens 90 diasa"


üåê 3. Extra√ß√£o de Dados da Web

Um artigo sobre o Microsoft Fabric √© carregado diretamente da web:

dsa_loader = WebBaseLoader("https://blog.dsacademy.com.br/...")
documentos = dsa_loader.load()

![image](https://github.com/user-attachments/assets/7151f53c-49e4-48a2-9138-a67939378156)



üß© 4. Divis√£o do Texto em Chunks

O texto √© dividido em partes menores (chunks) com RecursiveCharacterTextSplitter:

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1800, chunk_overlap=200)
docs = text_splitter.split_documents(documentos)

Isso ajuda a manter o contexto relevante e reutiliz√°vel na recupera√ß√£o vetorial.


üß† 5. Cria√ß√£o dos Embeddings

Os embeddings s√£o vetores num√©ricos que representam semanticamente cada chunk de texto:

embeddings = HuggingFaceEmbeddings(model_name = "BAAI/bge-small-en-v1.5")
Esse modelo √© pequeno, r√°pido e adequado para tarefas de similaridade sem√¢ntica.


üóÉ 6. Banco de Dados Vetorial (Milvus)

Os chunks com embeddings s√£o armazenados em um banco vetorial:

dsa_vector_db = Milvus.from_documents(...)

Permite buscas por similaridade vetorial com base nas perguntas feitas.


üîé 7. Recupera√ß√£o de Contexto

Com o .as_retriever(), criamos um mecanismo de busca sem√¢ntica:

retriever = dsa_vector_db.as_retriever()

E o testamos com similarity_search("O Que √© o Microsoft Fabric?").

![image](https://github.com/user-attachments/assets/8b45d6e2-fe80-4176-b69a-e99e770f7a6b)



ü§ñ 8. Defini√ß√£o do LLM do Databricks

Aqui voc√™ define qual modelo LLM ser√° usado para gerar as respostas:

llm = ChatDatabricks(endpoint = "databricks-dbrx-instruct", max_tokens = 200)

![image](https://github.com/user-attachments/assets/b3586dd6-2efc-4fed-9a39-3bdd0a283cf7)


üßæ 9. Cria√ß√£o do Prompt Template

Define a estrutura da pergunta que ser√° enviada ao modelo:

PROMPT_TEMPLATE = """..."""
prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "question"])
Esse prompt inclui um bloco <context> (com os dados recuperados) e <question> (a pergunta do usu√°rio).


üîÑ 10. Defini√ß√£o da RAG Chain

Aqui √© montada a "cadeia" de execu√ß√£o que conecta as etapas:

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

Explicando cada parte:

* retriever | format_docs: busca os documentos e formata em texto plano.

* RunnablePassthrough(): passa a pergunta diretamente.

* prompt: monta a entrada com base no contexto e na pergunta.

* llm: envia a entrada ao modelo do Databricks.

* StrOutputParser(): pega apenas a resposta em texto do modelo.


‚ñ∂Ô∏è 11. Execu√ß√£o do Pipeline
A pipeline √© invocada com .invoke():

resposta = rag_chain.invoke("Como Empresas Podem Utilizar o Microsoft Fabric?")

Isso retorna uma resposta gerada com base no texto recuperado da web + modelo LLM.

![image](https://github.com/user-attachments/assets/7605ecd9-8282-4698-ba8c-6cc0ecc30eca)


üß™ 12. Resultados e Verifica√ß√£o

Voc√™ faz novas perguntas e o pipeline responde de forma precisa, contextualizada e confi√°vel.

üìå Resumo Visual do Fluxo:

[Pergunta do Usu√°rio]

        ‚Üì
        
[Retriever busca os chunks relevantes no banco vetorial]

        ‚Üì
        
[Prompt √© preenchido com contexto + pergunta]

        ‚Üì
        
[LLM Databricks gera uma resposta com base nisso]

        ‚Üì
        
[Resposta Final]


# Abaixo est√° um diagrama simplificado do pipeline RAG que foi implementodo, agora vamos adapt√°-lo a outros dom√≠nios (como jur√≠dico ou financeiro).


üìä Diagrama do Pipeline RAG com LLM no Databricks

   +----------------------+
   
   | Pergunta do Usu√°rio  |

   +----------------------+
   
            ‚Üì
   +-------------------------------+
   
   | Recuperador (Retriever)      |
   
   | - Busca nos Embeddings       |
   
   | - Base Vetorial (Milvus)     |
   
   +-------------------------------+
   
            ‚Üì
   +-------------------------------+
   
   | Contexto Recuperado           |
   
   +-------------------------------+
   
            ‚Üì
   +-------------------------------+
   
   | Prompt Template               |
   
   | - Injeta pergunta e contexto |
   
   +-------------------------------+
   
            ‚Üì
   +----------------------------------------+
   
   | Modelo LLM no Databricks     |
   
   | - endpoint: "databricks-dbrx-instruct" |
   
   +----------------------------------------+
   
            ‚Üì
   +-------------------------------+
   
   | Resposta Gerada              |
  
   +-------------------------------+


üèõÔ∏è Como Adaptar o RAG para Dom√≠nios Espec√≠ficos

üîπ 1. Jur√≠dico

Objetivo: Criar um assistente jur√≠dico para consultas legais com base em leis atualizadas.

* Fonte de dados: sites de tribunais, portais jur√≠dicos, legisla√ß√£o (como o Planalto.gov.br ou JusBrasil).

* Prompt: instruir o LLM a responder com base em dispositivos legais.

* Exemplo de pergunta: ‚ÄúQuais s√£o os direitos do consumidor em caso de cancelamento de voo?‚Äù


üîπ 2. Financeiro

Objetivo: Responder d√∫vidas sobre produtos banc√°rios, investimentos ou normas da CVM/BC.

* Fonte de dados: sites do Banco Central, CVM, portais de not√≠cias financeiras (Valor, InfoMoney).

* Prompt: garantir respostas com base em regulamenta√ß√µes e dados reais.

* Exemplo de pergunta: ‚ÄúQuais as regras para resgatar um CDB antes do vencimento?‚Äù


üîπ 3. Sa√∫de

Objetivo: Oferecer respostas sobre doen√ßas, tratamentos, etc., com base em fontes m√©dicas confi√°veis.

* Fonte de dados: sites como Minist√©rio da Sa√∫de, OMS, PubMed.

* Prompt: alertar o modelo para n√£o dar conselhos m√©dicos finais (s√≥ informativos).

* Exemplo de pergunta: ‚ÄúQuais os sintomas da dengue?‚Äù


‚úÖ Conclus√£o

A implementa√ß√£o de um pipeline RAG com LLMs no Databricks representa um passo significativo na constru√ß√£o de aplica√ß√µes de IA Generativa mais robustas, precisas e seguras. Ao integrar recupera√ß√£o de informa√ß√µes em tempo real com modelos de linguagem poderosos, superamos limita√ß√µes tradicionais dos LLMs, como desatualiza√ß√£o dos dados e risco de alucina√ß√µes.

Durante este projeto, exploramos a estrutura t√©cnica por tr√°s do RAG, configuramos o ambiente com ferramentas como Milvus, LangChain, HuggingFace e Databricks, e demonstramos seu funcionamento com um exemplo pr√°tico baseado em dados da web. O resultado foi um sistema capaz de fornecer respostas contextualizadas e atualizadas, com rastreabilidade e flexibilidade para adapta√ß√£o a diferentes dom√≠nios, como jur√≠dico, financeiro e sa√∫de.

Com essa base, o pipeline pode ser facilmente adaptado e expandido para outras √°reas de neg√≥cio ou setores regulados, oferecendo solu√ß√µes inteligentes com confiabilidade e escalabilidade. O futuro da IA Generativa est√° cada vez mais orientado a aplica√ß√µes pr√°ticas com integra√ß√£o de dados din√¢micos ‚Äî e o RAG √© uma pe√ßa central nesse avan√ßo.


